# Neural Network ReLU Layer with Variable Binding
# Clean implementation of the original question using where clause binding

print("=== NEURAL RELU LAYER WITH VARIABLE BINDING ===");
print("Clean solution to: where res = sum[l](xxx)");
print();

# =============================================================================
# ORIGINAL PROBLEM SOLVED
# =============================================================================

# The original complex expression was:
# where (sum[l](input[i, l] * w1[l, k]) + b1[k]) > 0.0, 0.0

# Here's the clean solution using variable binding:

fn neural_relu_layer(input, w1, b1) {
    let output[i, k] = activated_value;
                       where linear_result = sum[l](input[i, l] * w1[l, k]) + b1[k],
                             activated_value = if linear_result > 0.0 { linear_result } else { 0.0 };
    #                        â†‘                                              â†‘
    #                  bind computation                              use bound variable
    
    output
}

# =============================================================================
# TEST THE SOLUTION
# =============================================================================

# Test data
let input_data: tensor[f32; 2, 3] = [[1.0, 2.0, 3.0], [-1.0, 0.5, 2.0]];
let weight_matrix: tensor[f32; 3, 4] = [
    [0.1, 0.2, -0.1, 0.3],
    [0.4, -0.2, 0.5, 0.1], 
    [-0.3, 0.6, 0.2, -0.4]
];
let bias_vector: tensor[f32; 4] = [0.1, -0.2, 0.3, 0.0];

let result = neural_relu_layer(input_data, weight_matrix, bias_vector);

print("Input shape: [2, 3] (2 samples, 3 features)");
print("Weight shape: [3, 4] (3 input features, 4 output neurons)");
print("Bias shape: [4] (4 output neurons)");
print();
print("Input data:", input_data);
print("ReLU layer output:", result);
print();

# =============================================================================
# COMPARISON: OLD VS NEW SYNTAX
# =============================================================================

print("=== SYNTAX COMPARISON ===");

print("âŒ Original problematic syntax:");
print("where (sum[l](input[i, l] * w1[l, k]) + b1[k]) > 0.0, 0.0");
print();

print("âœ… Clean variable binding solution:");
print("where linear_result = sum[l](input[i, l] * w1[l, k]) + b1[k],");
print("      activated_value = if linear_result > 0.0 { linear_result } else { 0.0 }");
print();

# =============================================================================
# ADVANCED EXAMPLE: MULTI-LAYER WITH BINDING
# =============================================================================

print("=== MULTI-LAYER NETWORK ===");

fn two_layer_relu_network(input, w1, b1, w2, b2) {
    # First layer with ReLU
    let hidden[i, j] = h_activated;
                       where h_linear = sum[k](input[i, k] * w1[k, j]) + b1[j],
                             h_activated = if h_linear > 0.0 { h_linear } else { 0.0 };
    
    # Second layer (output, no activation)
    let output[i, j] = sum[k](hidden[i, k] * w2[k, j]) + b2[j];
    
    output
}

# Network weights for 2-layer network
let w1: tensor[f32; 3, 4] = weight_matrix;  # Reuse from above;
let b1: tensor[f32; 4] = bias_vector;       # Reuse from above;
let w2: tensor[f32; 4, 2] = [[0.5, -0.3], [0.2, 0.7], [-0.4, 0.1], [0.6, -0.2]];
let b2: tensor[f32; 2] = [0.1, -0.1];

let network_output = two_layer_relu_network(input_data, w1, b1, w2, b2);

print("Two-layer network:");
print("  Input: [2, 3] -> Hidden: [2, 4] -> Output: [2, 2]");
print("  Final output:", network_output);
print();

# =============================================================================
# PERFORMANCE BENEFITS DEMONSTRATION
# =============================================================================

print("=== PERFORMANCE BENEFITS ===");

# Without binding (inefficient - repeats expensive computation)
fn inefficient_relu_layer(input, w1, b1) {
    let output[i, k] = if (sum[l](input[i, l] * w1[l, k]) + b1[k]) > 0.0 {
                           (sum[l](input[i, l] * w1[l, k]) + b1[k])
                       } else {
                           0.0
                       };
    #                     â†‘ COMPUTED TWICE! â†‘
    output
}

# With binding (efficient - computes once)
fn efficient_relu_layer(input, w1, b1) {
    let output[i, k] = activated;
                       where z = sum[l](input[i, l] * w1[l, k]) + b1[k],
                             activated = if z > 0.0 { z } else { 0.0 };
    #                        â†‘ COMPUTED ONCE! â†‘
    output
}

print("âœ… Efficient version computes sum[l](...) exactly once per element");
print("âŒ Inefficient version computes sum[l](...) twice per element");
print("ðŸš€ Variable binding = cleaner code + better performance!");
print();

# =============================================================================
# REAL-WORLD PATTERNS
# =============================================================================

print("=== REAL-WORLD PATTERNS ===");

# Leaky ReLU with binding
fn leaky_relu_layer(input, w1, b1, negative_slope) {
    let output[i, k] = leaky_activated;
                       where linear_val = sum[l](input[i, l] * w1[l, k]) + b1[k],
                             leaky_activated = if linear_val > 0.0 { 
                                                 linear_val 
                                               } else { 
                                                 negative_slope * linear_val 
                                               };
    output
}

# Batch normalization with binding (conceptual)
fn batch_norm_layer(input, gamma, beta) {
    let normalized[i, j] = final_output;
                           where raw_val = input[i, j],
                                 # Simplified batch norm (would need proper mean/var computation)
                                 centered = raw_val - 0.0,  # mean would go here;
                                 scaled = centered / 1.0,   # std would go here;
                                 gamma_scaled = scaled * gamma[j],
                                 final_output = gamma_scaled + beta[j];
    normalized
}

print("âœ… Leaky ReLU with variable binding");
print("âœ… Batch normalization pattern");
print("âœ… Complex activations made simple");
print();

print("=== SUMMARY ===");
print("ðŸŽ¯ Variable binding in where clauses:");
print("   â€¢ Makes complex expressions readable");
print("   â€¢ Avoids expensive computation repetition");
print("   â€¢ Enables step-by-step inspection");
print("   â€¢ Perfect for neural network layers");
print();
print("ðŸ”¥ Pattern: where var = computation, condition");
print("ðŸ’¡ Your original question solved elegantly!");
