# Computer Vision with Higher Rank Tensors
# Practical examples of 4D tensor operations in image processing and CNNs

use std::math::sqrt;

print("=== COMPUTER VISION TENSOR OPERATIONS ===");
print("Realistic image processing with 4D tensors (NCHW format)");
print();

# =============================================================================
# BATCH IMAGE PREPROCESSING
# =============================================================================

print("=== BATCH IMAGE PREPROCESSING ===");

# Simulate a batch of RGB images: (batch, channels, height, width)
let image_batch = [
    # Image 1: 3 channels (RGB), 4x4 pixels
    [
        [[10, 20, 30, 15], [25, 35, 45, 20], [40, 50, 60, 35], [55, 65, 75, 40]] as [f32], # Red channel
        [[15, 25, 35, 20], [30, 40, 50, 25], [45, 55, 65, 40], [60, 70, 80, 45]] as [f32], # Green channel  
        [[20, 30, 40, 25], [35, 45, 55, 30], [50, 60, 70, 45], [65, 75, 85, 50]] as [f32]  # Blue channel
    ],
    # Image 2: Same structure, different values
    [
        [[12, 22, 32, 17], [27, 37, 47, 22], [42, 52, 62, 37], [57, 67, 77, 42]] as [f32],
        [[17, 27, 37, 22], [32, 42, 52, 27], [47, 57, 67, 42], [62, 72, 82, 47]] as [f32],
        [[22, 32, 42, 27], [37, 47, 57, 32], [52, 62, 72, 47], [67, 77, 87, 52]] as [f32]
    ]
];  # Shape: (2, 3, 4, 4) - (batch=2, channels=3, height=4, width=4)

print("Image batch shape: (batch=2, channels=3, height=4, width=4)");
print("Image 1, Red channel:");
print("Row 0:", [image_batch[0,0,0,w] | w in 0..4]);
print("Row 1:", [image_batch[0,0,1,w] | w in 0..4]);
print("Row 2:", [image_batch[0,0,2,w] | w in 0..4]);
print("Row 3:", [image_batch[0,0,3,w] | w in 0..4]);
print();

# =============================================================================
# NORMALIZATION OPERATIONS
# =============================================================================

print("=== IMAGE NORMALIZATION ===");

# Per-channel statistics across the batch  
let channel_mean[c in 0..3] = sum[b,h,w](image_batch[b,c,h,w]) / ((2 * 4 * 4) as f32);  # Mean per channel
let channel_var[c in 0..3] = sum[b,h,w]((image_batch[b,c,h,w] - channel_mean[c]) *
                                (image_batch[b,c,h,w] - channel_mean[c])) / ((2 * 4 * 4) as f32);
let channel_std[c in 0..3] = sqrt(channel_var[c]);

print("Channel-wise statistics:");
print("RGB means:", channel_mean);
print("RGB standard deviations:", channel_std);
print();

# Apply normalization (ImageNet-style)
let imagenet_mean = [123.68, 116.78, 103.94];  # Typical ImageNet normalization values;
let imagenet_std = [58.40, 57.12, 57.38];

let normalized[b,c,h,w] = (image_batch[b,c,h,w] - imagenet_mean[c]) / imagenet_std[c];

print("Normalized image statistics:");
let norm_mean[c in 0..3] = sum[b,h,w](normalized[b,c,h,w]) / ((2 * 4 * 4) as f32);
print("Post-normalization means (should be ~0):", norm_mean);
print();

# =============================================================================
# CONVOLUTIONAL OPERATIONS
# =============================================================================

print("=== CONVOLUTIONAL LAYER SIMULATION ===");

# Define convolution kernels: (out_channels, in_channels, kernel_h, kernel_w)
let conv_weights = [
    # First output channel (edge detector)
    [
        [[-1, -1, -1], [0, 0, 0], [1, 1, 1]] as [f32],  # Red channel kernel
        [[-1, -1, -1], [0, 0, 0], [1, 1, 1]] as [f32],  # Green channel kernel  
        [[-1, -1, -1], [0, 0, 0], [1, 1, 1]] as [f32]   # Blue channel kernel
    ],
    # Second output channel (vertical edge)
    [
        [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]] as [f32],  # Red channel kernel
        [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]] as [f32],  # Green channel kernel
        [[-1, 0, 1], [-1, 0, 1], [-1, 0, 1]] as [f32]   # Blue channel kernel
    ]
];  # Shape: (2, 3, 3, 3) - (out_ch=2, in_ch=3, h=3, w=3)

let conv_bias = [0.1, -0.1];  # Bias per output channel

print("Convolution setup:");
print("Kernel shape: (out_channels=2, in_channels=3, height=3, width=3)");
print("Bias values:", conv_bias);
print();

# Perform convolution using Einstein notation 
# System should now infer kh<3, kw<3 from conv_weights[oc,ic,kh,kw] shape
let conv_output[b,oc,oh in 0..2,ow in 0..2] = sum[ic,kh,kw](
    normalized[b,ic,oh+kh,ow+kw] * conv_weights[oc,ic,kh,kw]
) + conv_bias[oc];  # Valid convolution: 4x4 -> 2x2

print("Convolution output shape: (batch=2, out_channels=2, height=2, width=2)");
print("Output channel 0, image 1:");
print("Row 0:", [conv_output[0,0,0,w] | w in 0..1]);
print("Row 1:", [conv_output[0,0,1,w] | w in 0..1]);
print("Output channel 1, image 1:");
print("Row 0:", [conv_output[0,1,0,w] | w in 0..1]);
print("Row 1:", [conv_output[0,1,1,w] | w in 0..1]);
print();

# =============================================================================
# POOLING OPERATIONS
# =============================================================================

print("=== POOLING OPERATIONS ===");

# Max pooling with 2x2 kernel
let max_pooled[b,c in 0..2] = max[h, w](conv_output[b,c,h,w]);  # Global max pooling

# Average pooling 
let avg_pooled[b,c in 0..2] = sum[h,w](conv_output[b,c,h,w]) / 4.0;  # Global average pooling

print("Pooling results:");
print("Max pooled (image 1):", max_pooled[0]);
print("Avg pooled (image 1):", avg_pooled[0]);
print();

# 2x2 pooling kernel - now using normalized tensor with 4x4 spatial dimensions
let quadrant_max[b,c,qh in 0..2,qw in 0..2] = max[h in 0..2, w in 0..2](normalized[b,c,qh+h,qw+w]);

print("2x2 pooling results (2x2 regions from 4x4 input):");
print("Image 1, channel 0:");
print("Row 0:", [quadrant_max[0,0,0,qw] | qw in 0..1]);
print("Row 1:", [quadrant_max[0,0,1,qw] | qw in 0..1]);
print("Image 1, channel 1:");
print("Row 0:", [quadrant_max[0,1,0,qw] | qw in 0..1]);
print("Row 1:", [quadrant_max[0,1,1,qw] | qw in 0..1]);
print();

# =============================================================================
# BATCH OPERATIONS
# =============================================================================

print("=== BATCH OPERATIONS ===");

# Batch statistics for batch normalization
let batch_channel_mean[c] = sum[b,h,w](conv_output[b,c,h,w]) / ((2 * 2 * 2) as f32);
let batch_channel_var[c] = sum[b,h,w]((conv_output[b,c,h,w] - batch_channel_mean[c]) * 
                                      (conv_output[b,c,h,w] - batch_channel_mean[c])) / ((2 * 2 * 2) as f32);

# Apply batch normalization
let gamma = [1.0, 1.0];  # Scale parameters;
let beta = [0.0, 0.0];   # Shift parameters;
let epsilon = 0.001;

let batch_normalized[b,c,h,w] = gamma[c] * (conv_output[b,c,h,w] - batch_channel_mean[c]) / 
                                sqrt(batch_channel_var[c] + epsilon) + beta[c];

print("Batch normalization:");
print("Pre-BN channel means:", batch_channel_mean);
print("Pre-BN channel variances:", batch_channel_var);

let post_bn_mean[c] = sum[b,h,w](batch_normalized[b,c,h,w]) / ((2 * 2 * 2) as f32);
print("Post-BN channel means (should be ~0):", post_bn_mean);
print();

# =============================================================================
# ADVANCED CNN OPERATIONS
# =============================================================================

print("=== ADVANCED CNN OPERATIONS ===");

# Depthwise convolution simulation (using 2x2 kernel to fit 2x2 input)
let depthwise_kernel = [
    [[1, -1], [1, -1]] as [f32],  # Channel 0 kernel (simplified 2x2)
    [[1, 1], [-1, -1]] as [f32]   # Channel 1 kernel (simplified 2x2)
];  # Shape: (2, 2, 2) - separate kernel per channel

let depthwise_output[b,c,oh in 0..1,ow in 0..1] = sum[kh in 0..2,kw in 0..2](
    batch_normalized[b,c,oh+kh,ow+kw] * depthwise_kernel[c,kh,kw]
);  # Single output position for demo (oh=0, ow=0)

print("Depthwise convolution output (1x1):");
print("Image 1:", [depthwise_output[0,c,0,0] | c in 0..1]);
print();

# Channel attention mechanism
let global_avg[b,c] = sum[h,w](batch_normalized[b,c,h,w]) / 4.0;  # Global average pooling;
let channel_attention[b,c in 0..2] = if global_avg[b,c] > 0.0 { 1.0 } else { 0.1 };  # Simplified activation;
let attended_features[b,c,h,w] = batch_normalized[b,c,h,w] * channel_attention[b,c];

print("Channel attention:");
print("Global averages (image 1):", global_avg[0]);
print("Attention weights (image 1):", channel_attention[0]);
print();

# =============================================================================
# MULTI-SCALE FEATURE EXTRACTION
# =============================================================================

print("=== MULTI-SCALE FEATURE EXTRACTION ===");

# Different receptive field sizes (simplified for 2x2 input)
let small_kernel = [[[1, -1]] as [f32], [[1, -1]] as [f32]];  # 1x2 kernel per channel  ;
let medium_kernel = [[[[1, -1], [1, -1]] as [f32], [[1, -1], [1, -1]] as [f32]], [[[1, 1], [-1, -1]] as [f32], [[1, 1], [-1, -1]] as [f32]]]; # 2x2 kernel per channel

# Multi-scale convolution (different kernel sizes)
let small_features[b,c,h in 0..2] = sum[ic,kw in 0..2](batch_normalized[b,ic,h,kw] * small_kernel[c,ic,kw]);  # 1D-like convolution
let medium_features[b,c] = sum[ic,kh in 0..2,kw in 0..2](batch_normalized[b,ic,kh,kw] * medium_kernel[c,ic,kh,kw]);  # 2x2 convolution

print("Multi-scale features:");
print("Small kernel output (image 1):", small_features[0]);
print("Medium kernel output (image 1):", medium_features[0]);
print();

# =============================================================================
# PERFORMANCE ANALYSIS
# =============================================================================

print("=== TENSOR OPERATION COMPLEXITY ===");

let total_params = 2 * 3 * 3 * 3 + 2;  # Conv weights + bias;
let total_ops = 2 * 2 * 2 * (3 * 3 * 3);  # Output size * kernel ops per position

print("Model complexity:");
print("Total parameters:", total_params);
print("Total operations (forward pass):", total_ops);
print("Memory usage (4D tensor): batch×channels×height×width bytes");
print();

# Feature map visualization (simplified)
let feature_magnitude[b,c] = sqrt(sum[h,w](attended_features[b,c,h,w] * 
                                           attended_features[b,c,h,w]));
let dominant_channel[b] = if feature_magnitude[b,0] > feature_magnitude[b,1] { 0 } else { 1 };

print("Feature analysis:");
print("Feature magnitudes (image 1):", feature_magnitude[0]);
print("Dominant channel (image 1):", dominant_channel[0]);
print();

print("=== COMPUTER VISION CONCEPTS DEMONSTRATED ===");
print("✓ NCHW tensor format: Industry standard for images");
print("✓ Batch processing: Efficient parallel computation");
print("✓ Multi-channel convolutions: RGB → feature maps");
print("✓ Normalization: ImageNet-style and batch normalization");
print("✓ Pooling operations: Max, average, and adaptive pooling");
print("✓ Advanced operations: Depthwise conv, channel attention");
print("✓ Multi-scale features: Different receptive field sizes");
print("✓ Performance analysis: Parameter and operation counting");
print();

print("=== COMPUTER VISION TENSOR OPERATIONS COMPLETE ===");
